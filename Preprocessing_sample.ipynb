{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4464c0cf-feaf-47c7-be5e-8deb059081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc4dff6-45b1-4786-a2df-e4436faaf854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab3114-cebc-4509-a578-1f9a54f76518",
   "metadata": {},
   "source": [
    "FILLING NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5d7f7f-c5ff-4c01-8273-86a6cb0ad3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro,ks_2samp,norm\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    # Attributes: df (Original DataFrame),  descriptor (Dataframe containing information about Null Values and Feature data type)\n",
    "    # Methods : Called from outside: __inti__(df), fillNull()\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df=df.copy()\n",
    "        self.descriptor=self.generateDescriptor()\n",
    "    \n",
    "    def generateDescriptor(self):    # Generates descriptor df\n",
    "        descriptor_df = pd.DataFrame(self.df.isnull().sum())\n",
    "        descriptor_df[1] = round(descriptor_df[0]/self.df.shape[0],2)*100\n",
    "        isNumerical=[]\n",
    "        for column in self.df.columns:\n",
    "            if self.df[column].dtype == 'int64' or self.df[column].dtype == 'float64':\n",
    "                isNumerical.append(1)\n",
    "            else:\n",
    "                isNumerical.append(0)\n",
    "        descriptor_df[2]=isNumerical\n",
    "        return descriptor_df\n",
    "\n",
    "    def checkDistribution(self,series):    # Labels feature distribution as Normal or Skewed\n",
    "        if abs(series.skew())<0.5:\n",
    "            return \"normal\"\n",
    "        else:\n",
    "            if series.shape[0]<5000:\n",
    "                stat,p = shapiro(series)\n",
    "            else:\n",
    "                mu,sigma=series.mean(),series.std()\n",
    "                stat,p = ks_2samp(series,norm.rvs(loc=mu,scale=sigma,size=len(series)))\n",
    "            if p>0.05:\n",
    "                return \"normal\"\n",
    "            else:\n",
    "                return \"skewed\"    \n",
    "\n",
    "    def is_id_column(self,feature_series):    # Checks if feature is ID type\n",
    "        value_range = feature_series.max() - feature_series.min()\n",
    "        unique_count = feature_series.nunique()\n",
    "        if unique_count == len(feature_series):\n",
    "            if abs(value_range - unique_count) < 2:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def knnImpute(self,df,descriptor_df,feature):    # KNN imputation based on categorical or numerical feature\n",
    "        neighbour_features = [\n",
    "            it for it in descriptor_df.index\n",
    "            if descriptor_df[0][it] == 0\n",
    "            and descriptor_df[2][it] == 1\n",
    "            and not self.is_id_column(df[it])\n",
    "            ]\n",
    "    \n",
    "        neighbour_df=df[neighbour_features]\n",
    "        non_null=neighbour_df[df[feature].notna()]\n",
    "        null=neighbour_df[df[feature].isna()]\n",
    "        norm_min=neighbour_df.min()\n",
    "        norm_range=neighbour_df.max()-neighbour_df.min()\n",
    "        norm_range.replace(0,1e-9,inplace=True)\n",
    "        non_null=(non_null-norm_min)/norm_range\n",
    "        null=(null-norm_min)/norm_range\n",
    "        distances = pd.DataFrame(cdist(null,non_null,metric='euclidean'))\n",
    "        k=math.ceil(math.sqrt(non_null.shape[0]))\n",
    "        k_nearest_indices= pd.DataFrame(np.argsort(distances,axis=1)).iloc[:,:k]\n",
    "        \n",
    "        for enumerated_null_index,df_index in enumerate(null.index):\n",
    "            enumerated_non_null_indices=k_nearest_indices.loc[enumerated_null_index]\n",
    "            true_df_indices=non_null.iloc[enumerated_non_null_indices].index\n",
    "            neighbour_values = df[feature].loc[true_df_indices]\n",
    "            if descriptor_df[2][feature]==1 and self.df[feature].nunique()/len(self.df[feature])>0.05:\n",
    "                df.loc[df_index, feature] = neighbour_values.mean()\n",
    "            else:\n",
    "                df.loc[df_index, feature] = neighbour_values.mode().iloc[0]\n",
    "        \n",
    "    def fillNull(self):\n",
    "        for feature in self.descriptor.index:\n",
    "            \n",
    "            if self.descriptor[1][feature]>30:    # Feature has more than 30% null values\n",
    "                self.df.drop(feature,axis=1,inplace=True)    # Remove feature\n",
    "                self.descriptor = self.generateDescriptor()    # Remake descriptor df\n",
    "                continue\n",
    "                \n",
    "            if self.descriptor[2][feature]==1:    # Numerical feature\n",
    "                \n",
    "                if self.descriptor[1][feature]>5:    # Null values between 5-30%\n",
    "                    self.knnImpute(self.df,self.descriptor,feature)    # Use KNN\n",
    "                    \n",
    "                else:    # Null values between less than 5%\n",
    "                    if self.df[feature].nunique()/len(self.df[feature])<0.05:    # Ordinal Feature (Discrete Finite numerical)\n",
    "                        self.df.loc[:, feature] = self.df[feature].fillna(self.df[feature].mode().iloc[0]).copy()   # Fill with mode\n",
    "                        \n",
    "                    elif self.checkDistribution(self.df[feature]) == 'normal':    # Normal numerical fetaure\n",
    "                        self.df.loc[:, feature] = self.df[feature].fillna(self.df[feature].mean()).copy()    # Fill with mean\n",
    "                        \n",
    "                    else:    # Skewed numerical feature\n",
    "                        self.df.loc[:, feature] = self.df[feature].fillna(self.df[feature].median()).copy()    # Fill with median\n",
    "                        \n",
    "            else:    # Categorical feature\n",
    "                if self.descriptor[1][feature]>5:    # Null values between 5-30%\n",
    "                    self.knnImpute(self.df,self.descriptor,feature)    # Use KNN\n",
    "                    \n",
    "                else:    # Null values between less than 5%\n",
    "                    self.df.loc[:, feature] = self.df[feature].fillna(self.df[feature].mode().iloc[0]).copy()   # Fill with mode\n",
    "\n",
    "                    \n",
    "        self.descriptor=self.generateDescriptor()    # Remake descriptor df\n",
    "        return self\n",
    "\n",
    "    def transform(self):\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb400d5c-8eae-4b25-a367-5465ea1732ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre=Preprocessor(df)\n",
    "df1=pre.fillNull().transform()\n",
    "df1.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9a82c-7511-4e02-b40d-d42c1a2ecc8a",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60690cf-6ede-49ca-8069-b9b74b1352c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureEngineering:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def safe_parse_date(date_str):\n",
    "        try:\n",
    "            return parser.parse(date_str)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def extract_datetime_features(self):\n",
    "        datetime_cols = self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        datetime_cols = [col for col in datetime_cols if self.df[col].str.contains(r'\\d', na=False, regex=True).any()]\n",
    "        \n",
    "        new_features = {}\n",
    "\n",
    "        for col in datetime_cols:\n",
    "            self.df[col] = self.df[col].apply(lambda x: FeatureEngineering.safe_parse_date(x) if pd.notna(x) else None)\n",
    "            self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "\n",
    "            valid_rows = self.df[col].notna()\n",
    "            if valid_rows.sum() < 0.3 * self.df.shape[0]:  # Keep only columns with enough valid dates\n",
    "                continue\n",
    "\n",
    "            new_features[f\"{col}_year\"] = self.df[col].dt.year\n",
    "            new_features[f\"{col}_month\"] = self.df[col].dt.month\n",
    "            new_features[f\"{col}_day\"] = self.df[col].dt.day\n",
    "            new_features[f\"{col}_weekday\"] = self.df[col].dt.weekday\n",
    "            new_features[f\"{col}_hour\"] = self.df[col].dt.hour\n",
    "            self.df.drop(columns=[col], inplace=True)\n",
    "\n",
    "        if new_features:\n",
    "            self.df = pd.concat([self.df, pd.DataFrame(new_features, index=self.df.index)], axis=1)  # Efficient joining\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extract_text_features(self):\n",
    "        text_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        new_features = {}\n",
    "\n",
    "        for col in text_cols:\n",
    "            if self.df[col].isna().all():\n",
    "                continue\n",
    "            new_features[f\"{col}_char_count\"] = self.df[col].astype(str).apply(len)\n",
    "            new_features[f\"{col}_word_count\"] = self.df[col].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "        if new_features:\n",
    "            self.df = self.df.assign(**new_features)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_interaction_features(self):\n",
    "        num_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        new_features = {}\n",
    "    \n",
    "        if len(num_cols) > 1:\n",
    "            for i in range(len(num_cols)):\n",
    "                for j in range(i + 1, len(num_cols)):\n",
    "                    col1, col2 = num_cols[i], num_cols[j]\n",
    "    \n",
    "                    # Basic multiplicative interaction\n",
    "                    new_features[f\"{col1}_x_{col2}\"] = self.df[col1].fillna(0) * self.df[col2].fillna(0)\n",
    "                    \n",
    "                    # Additive interaction\n",
    "                    new_features[f\"{col1}_plus_{col2}\"] = self.df[col1].fillna(0) + self.df[col2].fillna(0)\n",
    "                    \n",
    "                    # Ratio interaction (avoid division by zero)\n",
    "                    new_features[f\"{col1}_div_{col2}\"] = self.df[col1] / (self.df[col2] + 1e-9)\n",
    "                    new_features[f\"{col2}_div_{col1}\"] = self.df[col2] / (self.df[col1] + 1e-9)\n",
    "    \n",
    "        if new_features:\n",
    "            self.df = pd.concat([self.df, pd.DataFrame(new_features, index=self.df.index)], axis=1).copy()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def create_statistical_features(self):\n",
    "        num_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        if len(num_cols) == 0:\n",
    "            return self\n",
    "\n",
    "        self.df[\"num_mean\"] = self.df[num_cols].mean(axis=1)\n",
    "        self.df[\"num_std\"] = self.df[num_cols].std(axis=1)\n",
    "        self.df[\"num_median\"] = self.df[num_cols].median(axis=1)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def encode_categorical_features(self):\n",
    "        cat_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        new_features = {}\n",
    "\n",
    "        for col in cat_cols:\n",
    "            counts = self.df[col].value_counts().to_dict()\n",
    "            freqs = self.df[col].map(self.df[col].value_counts(normalize=True))\n",
    "            \n",
    "            new_features[f\"{col}_count\"] = counts\n",
    "            new_features[f\"{col}_freq\"] = freqs\n",
    "\n",
    "        if new_features:\n",
    "            self.df = self.df.assign(**new_features)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self):\n",
    "        self.df.dropna(axis=1, how='any', inplace=True)  # Drops all columns with any NaN values\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def automated_feature_engineering(self):\n",
    "        return (\n",
    "            self.extract_datetime_features()\n",
    "            .extract_text_features()\n",
    "            .create_interaction_features()\n",
    "            .create_statistical_features()\n",
    "            .encode_categorical_features()\n",
    "            .transform()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cb1d8a-7811-45cd-b38d-c78e2688e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineering(df1)\n",
    "df2 = fe.automated_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d76c06-a688-4019-9745-306f29ddfdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 334)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3309f69-0956-4da0-98c2-ff1473fc8de3",
   "metadata": {},
   "source": [
    "ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40530afc-911a-437a-80bf-e42e2b96af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self,df):\n",
    "        self.df=df.copy()\n",
    "        self.encoding_type=self.generateEncodingType()\n",
    "\n",
    "    def generateEncodingType(self): \n",
    "        categorical_columns=self.df.select_dtypes(include=['object']).columns.tolist()\n",
    "        encoding_type={}\n",
    "        for column in categorical_columns:\n",
    "            if self.df[column].nunique()<10:\n",
    "                encoding_type[column]='OHE'\n",
    "            else:\n",
    "                encoding_type[column]='frequency'\n",
    "        return encoding_type\n",
    "\n",
    "    def oneHotEncoding(self,feature): \n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce08e9d-d570-4ffc-bd5d-5bd5c57466df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'frequency', 'Sex': 'OHE', 'Embarked': 'OHE'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en=Encoder(df2)\n",
    "en.encoding_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4d46d-f8ef-47c8-bfab-9870a84cc38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9cc03-8b5b-49a7-9ae2-45086f84effc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
